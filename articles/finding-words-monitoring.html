<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Finding the Right Words for Monitoring — Miguel Tomás Gomes</title>
  <meta name="description" content="UX writing case study by Miguel Tomás Gomes." />

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://migueltomasgomes.com/articles/finding-words-monitoring">
  <meta property="og:title" content="Finding the Right Words for Monitoring">
  <meta property="og:description" content="Standardized terminology in a new Monitoring feature through user testing and statistical analysis, reducing support tickets by up to 27%.">
  <meta property="og:image" content="https://migueltomasgomes.com/assets/cover-monitoring.png">

  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:url" content="https://migueltomasgomes.com/articles/finding-words-monitoring">
  <meta property="twitter:title" content="Finding the Right Words for Monitoring">
  <meta property="twitter:description" content="Standardized terminology in a new Monitoring feature through user testing and statistical analysis, reducing support tickets by up to 27%.">
  <meta property="twitter:image" content="https://migueltomasgomes.com/assets/cover-monitoring.png">

  <!-- Canonical URL -->
  <link rel="canonical" href="https://migueltomasgomes.com/articles/finding-words-monitoring">

  <link rel="stylesheet" href="../styles.css?v=37" />
  <link rel="icon" type="image/png" href="../assets/favicon-miguel-tomas-gomes.png">
</head>
<body>
  <header class="site-header">
    <div class="container">
      <nav class="nav" aria-label="Primary">
        <a href="/" class="brand">Miguel&nbsp;Tomás&nbsp;Gomes</a>
        <ul class="nav-links">
          <li><a href="/about">About</a></li>
          <li><a href="/articles/">Work Samples</a></li>
          <li><a href="../#contact">Contact</a></li>
        </ul>
      </nav>
    </div>
  </header>

  <main id="main">
    <!-- ARTICLE CONTENT -->
    <article class="article-content">
      <div class="container article-body">
        <!-- TITLE AND META -->
        <div class="article-header">
          <h1>Finding the Right Words for Monitoring</h1>
          <p class="article-meta">UX Writing • Work Sample • 2023 • 6 min read</p>
        </div>
        
        <!-- IMAGE WITHIN MARGINS -->
        <div class="article-image hero-with-tldr">
          <img src="../assets/cover-monitoring.png" alt="UX writing Work sample preview" width="800" height="400" />

          <!-- TL;DR Blocks Overlay -->
          <div class="tldr-blocks">
            <div class="tldr-block">
              <div class="tldr-header">Problem</div>
              <div class="tldr-title">Terminology confusion</div>
              <div class="tldr-subtitle">Internal teams using inconsistent language across UI, docs, and support</div>
            </div>
            <div class="tldr-block">
              <div class="tldr-header">Solution</div>
              <div class="tldr-title">User research study</div>
              <div class="tldr-subtitle">40-participant terminology testing with chi-square validation and weighted scoring</div>
            </div>
            <div class="tldr-block">
              <div class="tldr-header">Result</div>
              <div class="tldr-title">27% fewer support tickets</div>
              <div class="tldr-subtitle">Standardized monitoring vocabulary with measurable business impact</div>
            </div>
          </div>
        </div>

        <section>
          <p>In enterprise software, the wrong words can create internal friction and drive up support tickets. This project set out to standardize the terminology in a new Monitoring feature. What looked like a mere naming decision revealed itself to be an alignment between developers, admins, and product owners.</p>

          <div class="methodology-note">
            <h4>Methodology Note</h4>
            <ul>
              <li><strong>Company:</strong> Anonymized for confidentiality reasons</li>
              <li><strong>Data:</strong> Two rounds of unmoderated testing with 40 participants across technical roles</li>
              <li><strong>Purpose:</strong> Demonstrate terminology standardization methodology</li>
              <li><strong>Tools:</strong> UserTesting, Figma</li>
              <li><strong>Target users:</strong> Developers, admins, and product owners</li>
            </ul>
          </div>

          <h2>The Challenge</h2>
	  <p>The Monitoring feature was designed to display logs and system traces. Some of these terms had roots in industry standards, others were unique to our product. None, however, worked consistently across our users.</p>
	  <p>Developers leaned toward precision, while product owners cared more about accessible language. Admins, on the other hand, wanted both.</p>
          <p>In our product, the concept of a trace was labeled as "Activity". Additionally, what is commonly known as spans appeared in the UI as "Activity details". Industry-wise, however, these words have more precise meanings.</p>
	  <p>For instance, a trace is the complete record of how an application executes logic across services, while a span is a single sub-operation within that trace. Spans are defined by their start and end time, duration, and metadata. As such, they provide context to a trace.</p>
	  <img src="../assets/monitoring-terms.png" alt="screen showing a trace and spans" class="article-image-spaced" />
	  <p>The mismatch between our nomenclature and industry-wide terminology created ambiguity for technical and non-technical users, making it clear to us that both needed to be reconsidered.</p>
	  <p>Even internally, teams didn't fully agree on what each term meant. This inconsistency made its way into the UI and documentation, creating friction and confusion.</p>
	  <p>I needed to find terminology that was natural, widely understood, and still accurate enough to preserve credibility with technical users.</p>
	  <h2>The Approach</h2>
	  <p>As the only UX Writer assigned to this project, I developed and ran two rounds of unmoderated testing with a total of 40 participants of varied technical roles. Each session followed the same pattern: first, participants answered a free-input prompt to capture their natural vocabulary.</p>
    <p>Then, they saw a randomized multiple-choice set of four options, including their own input when possible. Finally, they were instructed to explain their reasoning in a free-input field.</p>
	  <h3>Methodology</h3>	  
	  <p>Splitting the test into two rounds helped control for context bias. The first round used text-only prompts and the second used UI mockups. Text-only prompts asked users to pick the most intuitive option for a given definition, while UI mockups added visual cues and a missing term with multiple options.</p>
    <p>This way, I could see whether a term felt natural in isolation but confusing in the interface, or vice versa. To ensure statistical rigor, I added validation in two ways. The percentages reported in findings reflect the UI mockup round, as this better predicted real-world performance in our product interface.</p>
	  <p>First, I used chi-square tests to check whether the overall preference patterns differed from random chance, then binomial tests to validate specific choices. For example, with four possible terms, random selection would predict 25% each; with three options, 33% each; with two options, 50% each.</p>
	  <p>Second, I used binomial tests to compare the top two options in each category, testing whether the leading choice significantly outperformed its closest competitor.</p>
	  <p>I also introduced a weighted scoring system: terms we internally found to be widespread and recognized as industry standards received a multiplier of +1.25. </p>
	  <p>Terms that caused hesitation or confusion were penalized with a 0.9 factor (-10%). This blend ensured that the final choices weren't driven by raw percentages alone, but by a balance of clarity and business strategy.</p>

	  <p>To illustrate how this worked in practice, here's an example calculation:</p>

	  <table class="scoring-table">
	    <thead>
	      <tr>
	        <th>Criterion</th>
	        <th>Base Score (Percentage)</th>
	        <th>Industry Multiplier</th>
	        <th>After Multiplier</th>
	        <th>Confusion Penalty</th>
	        <th>Final Score</th>
	      </tr>
	    </thead>
	    <tbody>
	      <tr>
	        <td>Example Term</td>
	        <td>6.0 (60%)</td>
	        <td>×1.25</td>
	        <td>7.5</td>
	        <td>×0.9</td>
	        <td>6.75</td>
	      </tr>
	    </tbody>
	  </table>
 	  <h2>Findings</h2>
		  <p>The following results reflect participant preferences when shown terms in UI context, which proved most predictive of actual usage patterns.</p>
	  <p>The first question was whether "Monitoring" worked as the umbrella term. Other options included Debugging, Analytics, and Insights. When shown in UI context, 24 of 40 participants chose "Monitoring" (60%), followed by 8 for "Debugging" (20%), 6 for "Analytics" (15%), and 2 for "Insights" (5%).</p>
	  <img src="../assets/monitoring-term.png" alt="monitoring results pie chart" class="article-image-spaced" />
	  <p>A chi-square test confirmed that preferences weren't random (p < 0.001), and a binomial test showed "Monitoring" significantly outperformed "Debugging" (24 vs. 8, p < 0.001). With its industry-standard multiplier, "Monitoring" clearly outperformed all alternatives.</p>
	  <p><b>Decision</b>: Keep "Monitoring" as the umbrella term. Reserve "Debugging" for troubleshooting contexts</p>
	  <h3>Logs</h3>
	  <p>The next test was simpler. Participants were asked what they would call a system-generated record of events. In the UI mockup round, thirty-six out of forty chose "Logs" (90%), with only four choosing anything else.</p>
	  <p>A binomial test confirmed that this preference was far stronger than the 50% baseline expected under chance (p < 0.001; 95% CI for the difference vs. baseline +30 to +62 percentage points). Free-input responses also overwhelmingly used "Logs".</p>
	  <p><b>Decision</b>: Keep "Logs" unchanged.</p>
	  <h3>Activities, Events, Traces</h3>
	  <p>The more difficult naming challenge emerged when we looked at how the product described a record of system behavior through time. In observability, the industry-standard term is a trace: a record that captures the sequence of events as an application executes logic, showing how different services interact.</p>
	  <p>Our UI, however, labeled this concept "Activities". On paper, the word sounded approachable. In practice, it was vague. Some participants assumed it meant audit logs of end-user actions, others were unsure whether it referred to system-level events.</p>
	  <p>In text-only prompts, many leaned toward “Events,” finding it intuitive and less technical than either Activities or Traces. But once shown UI mockups, comprehension shifted. With context, “Traces” gained ground. By the end of the second round, 24 of 40 participants (60%) favored “Traces”, compared with 2 for “Activities” (5%) and 14 (35%) for “Events”.</p>
	  <img src="../assets/monitoring-pie-chart.png" alt="traces results pie chart" class="article-image-spaced" />
	  <p>A chi-square test confirmed that preferences weren't random (p < 0.01), and "Traces" was chosen significantly more often than "Events" in head-to-head comparison (24 vs. 14, p < 0.05).</p>
	  <p>"Events" remained useful in specific contexts. Product owners found it approachable, and admins saw it as a good fit for user-facing occurrences. But under the scoring model, "Traces" carried the industry-standard multiplier, which pushed it above "Events" in the final evaluation.</p>
	  <p><b>Decision</b>: Retire "Activities". Keep "Traces" as the canonical industry term, supported by UI context and documentation. Use "Events" only for user-facing occurrences, not system-level tracing.</p>
	  <h3>Spans</h3>
	  <p>The final terminology challenge centered on "span", the unit inside a trace. A span is a sub-operation with its duration and metadata. Multiple spans, linked together, form a trace that shows how a request flows across services.</p>
	  <p>But the term span proved nearly unusable at first. Few participants recognized it. Non-technical users confused it with HTML tags, and only a handful of developers understood it correctly.</p>
	  <p>When participants were asked what they would call this unit in UI context, 28 out of 40 chose Operation (70%), 8 chose Job (20%), and only 4 chose Span (10%).</p>
	  <img src="../assets/span-chart.png" alt="spans results pie chart" class="article-image-spaced" />
	  <p>A chi-square test showed clear preferences beyond chance (p < 0.001), with "Operation" dominating the field. A binomial test confirmed "Operation" significantly outperformed "Job" (28 vs. 8, p < 0.001).</p>
	  <p>Under the scoring model, "Span" was penalized heavily for lack of comprehension, while Operation gained both industry credibility and cross-role clarity.</p>
	  <p>To preserve accuracy without alienating users, I recommended renaming it to "Sub-operation (span)" in the UI, paired with a tooltip explanation to educate gradually.</p>
	  <img src="../assets/span-tooltip.png" alt="screen showing tooltip on a span visual element" class="article-image-spaced" />
	  <p><b>Decision</b>: Replace "Span" with "Sub-operation (span)", supported by tooltips.</p>
          <h2>Outcome</h2>
	  <p>By the end of the research, the terminology set for the monitoring feature was standardized:</p>
	   <ul>
	     <li>Monitoring as the umbrella label.</li>
	    <li>Logs remain unchanged.</li>
	    <li>Activities retired in favor of traces, with events reserved for user-facing contexts.</li>
	    <li>Span reframed as sub-operation (span) with tooltip support.</li>
	   </ul>
          <p>This consistency carried through the UI, documentation, and support, reducing ambiguity and improving usability. Developers, admins, and product owners now had a shared language.</p>
          <h3>Results</h3>
	          <p>In follow-up interviews, participants described the revised terms as more natural and easier to navigate. I also touched base with support management three months after the changes went live. They reported up to 27% drop in tickets related to Monitoring.</p>
            <p>More importantly, the methodology set a precedent. By combining free input with multiple choice, text-only prompts with UI context, and validating results through statistical testing and weighted scoring, the team gained a repeatable model for terminology research.</p>
            <p>This outcome proved that words are not surface-level. They are fundamental to how everyone understands and interacts with a system.</p>
        </section>

        <footer class="article-footer">
          <p><strong>Enjoyed reading? </strong><a href="../#contact">Get in touch</a> with me or see more of <a href="/articles/">my work</a>.</p>
        </footer>
      </div>
    </article>
  </main>

<footer class="site-footer">
    <div class="container footer-grid">
      <a class="link" href="/articles/">View Work Samples</a>
      <p>© 2025 Miguel Tomás Gomes</p>
    </div>
  </footer>
</body>
</html>
